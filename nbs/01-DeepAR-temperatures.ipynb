{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting temperature time-series with the DeepAR built-in algorithm on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://berkeleyearth.org/data/\n",
    "\n",
    "The data set we're going to work with holds a daily temperature measure from 1880 to 2014: http://berkeleyearth.lbl.gov/auto/Global/Complete_TAVG_daily.txt\n",
    "\n",
    "Temperatures are reported as a delta from the 1951-1980 average (8.68Â°C)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and pre-process data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://berkeleyearth.lbl.gov/auto/Global/Complete_TAVG_daily.txt\n",
    "\n",
    "# Remove header lines (starting with a %), empty lines and lines with only spaces\n",
    "!grep -v -e '^%\\|^$\\|^\\ *$' Complete_TAVG_daily.txt > temps.txt\n",
    "!head -10 temps.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minYear  = 1880\n",
    "maxYear  = 2014\n",
    "avg_temp = 8.68\n",
    "\n",
    "# Our model will predict temperature for the next 'prediction_length' days\n",
    "prediction_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, boto3, json, sagemaker\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('temps.txt', 'r')\n",
    "data = csv.reader(f,delimiter=' ')\n",
    "\n",
    "dataset={}\n",
    "x=[]\n",
    "y=[]\n",
    "count=1\n",
    "prevYear=0\n",
    "\n",
    "for row in data:\n",
    "        # Remove empty strings caused by multiple spaces between columns\n",
    "        row = filter(None, row)\n",
    "        \n",
    "        year=row[1]\n",
    "        temp=float(row[5])+avg_temp\n",
    "         \n",
    "        # Data for plotting\n",
    "        # x list=counter, y list=temperature\n",
    "        x.append(count)\n",
    "        y.append(float(temp))\n",
    "        count=count+1\n",
    "        \n",
    "        # Data for training\n",
    "        # dictionary: key=year, value=list of ordered daily temperatures\n",
    "        if (year != prevYear):\n",
    "            dataset[year]=[]\n",
    "            prevYear=year\n",
    "        dataset[year].append(float(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_samples_per_year = list(map(lambda x: len(x), (dataset[str(year)] for year in range(minYear, maxYear+1))))\n",
    "nb_samples_per_year = np.unique(nb_samples_per_year).tolist()\n",
    "assert nb_samples_per_year == [365, 366]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nbSamples=len(x)\n",
    "print('Number of samples: %d' % nbSamples)\n",
    "\n",
    "fig=plt.figure(figsize=(64, 16))\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html\n",
    "# - remove the last 'prediction_length' points from each time series\n",
    "# - use the full data set for testing\n",
    "# - no need to shuffle the training set: dictionaries are not ordered :)\n",
    "\n",
    "trainingSet = dataset.copy()\n",
    "trainingSet[year] = { year: dataset[year][:-prediction_length] for year in dataset.keys() }\n",
    "\n",
    "testSet = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silly hack until I figure out what the problem is with 2014...\n",
    "trainingSet.pop('2014', None)\n",
    "testSet.pop('2014', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_key      = 'deepar_training.json'\n",
    "test_key       = 'deepar_test.json'\n",
    "\n",
    "def writeDataset(filename, data): \n",
    "    file=open(filename,'w')\n",
    "    for year in data.keys():\n",
    "        # One JSON sample per line\n",
    "        line = \"\\\"start\\\":\\\"{}-01-01 00:00:00\\\",\\\"target\\\":{}\".format(year,data[year])\n",
    "        file.write('{'+line+'}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writeDataset(train_key, trainingSet)        \n",
    "writeDataset(test_key, testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!head -20 deepar_training.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload training set and test set to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket         = 'jsimon-sagemaker-us'\n",
    "prefix         = 'sagemaker/deepar-daily-temperature'\n",
    "\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test')\n",
    "output_prefix  = '{}/{}'.format(prefix, 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role              = sagemaker.get_execution_role()\n",
    "region            = boto3.Session().region_name\n",
    "\n",
    "train_path  = sagemaker_session.upload_data(train_key, bucket=bucket, key_prefix=train_prefix)\n",
    "test_path   = sagemaker_session.upload_data(test_key,  bucket=bucket, key_prefix=test_prefix)\n",
    "output_path = 's3://{}/{}'.format(bucket, output_prefix)\n",
    "\n",
    "print(train_path)\n",
    "print(test_path)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "containers = {\n",
    "    'us-east-1': '522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:latest',\n",
    "    'us-east-2': '566113047672.dkr.ecr.us-east-2.amazonaws.com/forecasting-deepar:latest',\n",
    "    'us-west-2': '156387875391.dkr.ecr.us-west-2.amazonaws.com/forecasting-deepar:latest',\n",
    "    'eu-west-1': '224300973850.dkr.ecr.eu-west-1.amazonaws.com/forecasting-deepar:latest'\n",
    "}\n",
    "\n",
    "image_name = containers[region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.8xlarge',\n",
    "    base_job_name='daily-temperature',\n",
    "    output_path=output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html\n",
    "\n",
    "hyperparameters = {\n",
    "    \"time_freq\": 'D', # daily series\n",
    "    \"context_length\": prediction_length,\n",
    "    \"prediction_length\": prediction_length, # number of data points to predict\n",
    "    \"num_cells\": \"40\",\n",
    "    \"num_layers\": \"2\",\n",
    "    \"likelihood\": \"gaussian\",\n",
    "    \"epochs\": \"250\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"0.00001\",\n",
    "    \"dropout_rate\": \"0.05\",\n",
    "    \"early_stopping_patience\": \"10\" # stop if loss hasn't improved in 10 epochs\n",
    "}\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_path)\n",
    "print(test_path)\n",
    "print(output_path)\n",
    "\n",
    "data_channels = {\"train\": train_path, \"test\": test_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "job_name = estimator.latest_training_job.name\n",
    "\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    deployment_image=image_name,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "predictor = sagemaker.predictor.RealTimePredictor(\n",
    "    endpoint_name, \n",
    "    sagemaker_session=sagemaker_session, \n",
    "    content_type=\"application/json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build JSON-formatted prediction request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1 = '0.1'         # compute p10 quantile\n",
    "q2 = '0.9'         # compute p90 quantile\n",
    "num_samples = 100  # predict 100 sample series\n",
    "    \n",
    "def buildPredictionData(year, data):\n",
    "    year_temps = data[str(year)]\n",
    "    s = {\"start\": \"{}-01-01 00:00:00\".format(year), \"target\": year_temps}\n",
    "    series = []\n",
    "    series.append(s)\n",
    "    configuration = {\n",
    "        \"output_types\": [\"mean\", \"quantiles\", \"samples\"],\n",
    "        \"num_samples\": num_samples,\n",
    "        \"quantiles\": [q1, q2]\n",
    "    }\n",
    "    http_data = {\n",
    "        \"instances\": series, \n",
    "        \"configuration\": configuration\n",
    "    }\n",
    "    return json.dumps(http_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predicted series from response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getPredictedSeries(result):\n",
    "    import random\n",
    "    json_result = json.loads(result)\n",
    "    y_data      = json_result['predictions'][0]\n",
    "    y_mean      = y_data['mean']\n",
    "    y_q1        = y_data['quantiles'][q1]\n",
    "    y_q2        = y_data['quantiles'][q2]\n",
    "    y_sample    = y_data['samples'][random.randint(0, num_samples)]\n",
    "\n",
    "    #print(\"Mean: %s\\n\" % y_mean)\n",
    "    #print(\"Quartile %s: %s\\n\" % (q1, y_q1))\n",
    "    #print(\"Quartile %s: %s\\n\" % (q2, y_q2))\n",
    "    return y_mean, y_q1, y_q2, y_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot predicted series and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotSeries(result, truth=False, truth_data=None, truth_label=None):\n",
    "    x = range(0,prediction_length)\n",
    "    y_mean, y_q1, y_q2, y_sample = getPredictedSeries(result)\n",
    "    plt.gcf().clear()\n",
    "    mean_label,   = plt.plot(x, y_mean, label='mean')\n",
    "    q1_label,     = plt.plot(x, y_q1, label=q1)\n",
    "    q2_label,     = plt.plot(x, y_q2, label=q2)\n",
    "    sample_label, = plt.plot(x, y_sample, label='sample')\n",
    "\n",
    "    if truth:\n",
    "        ground_truth, = plt.plot(x, truth_data, label=truth_label)\n",
    "        plt.legend(handles=[ground_truth, q2_label, mean_label, q1_label, sample_label])\n",
    "    else:\n",
    "        plt.legend(handles=[q2_label, mean_label, q1_label, sample_label])\n",
    "    plt.yticks(np.arange(5.0, 12.0, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: predict temperature for the last 'prediction_length' days and compare to ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year = 1984 # âHe who controls the past controls the future. He who controls the present controls the past.â\n",
    "\n",
    "prediction_data = buildPredictionData(year, trainingSet)\n",
    "\n",
    "result = predictor.predict(prediction_data).encode('utf-8')\n",
    "\n",
    "plotSeries(result, \n",
    "           truth=True, \n",
    "           truth_data=testSet[str(year)][-prediction_length:], \n",
    "           truth_label='truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: predict temperature for the next 'prediction_length' days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict temperature for the next 'prediction_length' days of 2018\n",
    "year = 2018\n",
    "year_temps={}\n",
    "year_temps[str(year)] = np.random.normal(9, 1.5, 90).tolist()\n",
    "\n",
    "prediction_data = buildPredictionData(year, year_temps)\n",
    "result = predictor.predict(prediction_data).encode('utf-8')\n",
    "plotSeries(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job_name)\n",
    "sm = boto3.client('sagemaker')\n",
    "sm.delete_endpoint(EndpointName=job_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=job_name)\n",
    "sm.delete_model(ModelName=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python2",
   "language": "python",
   "name": "conda_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
